<!DOCTYPE html>
<html>
<head>
  <meta charset=utf-8>
  <meta name=viewport content="width=device-width, initial-scale=1.0">
  <meta name=description content="Personal website and blog of Umanga Bista">

  <title>
    
    Improving performance of Local outlier factor with KD-Trees | 
    
    Umanga Bista
  </title>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
  </script>

  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-50991800-1', 'bistaumanga.com.np');
  ga('send', 'pageview');

  </script>

  <link rel=stylesheet type=text/css href=/css/github.css>
  <link rel=stylesheet type=text/css href=/css/styles.css>
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel=stylesheet type=text/css href=/css/pure-min.css>
</head>

<body>


    <div class="top-bar pure-g">
      <div class=pure-u-1-3>
        <a href="http://bistaumanga.com.np">
          <img id="logo" src="/images/logo.svg" alt="" />
        </a>
      </div>

      <div class=pure-u-1-3>
        
      </div>

      <div class=pure-u-1-3>
        <div class="pure-menu pure-menu-open pure-menu-horizontal">
          <ul>
            <li class="pure-menu-selected"><a href="/"><h4><i class="fa fa-home fa-3x"></i> </h4></a></li>
            <li class="pure-menu-selected"><a href="/blog"><h4><i class="fa fa-pencil fa-3x"></i></h4></a></li>
            <li class="pure-menu-selected"><a href="/publications"><h4><i class="fa fa-file-text fa-3x"></i></h4></a></li>
            <li class="pure-menu-selected"><a href="/about"><h4><i class="fa fa-info fa-3x"></i></h4></a></li>
          </ul>
        </div>
      </div>
    </div>


  <div class="container pure-g">
    <div class=pure-u-4-5>
      <div class=right-column>
        <div class=post>
    
    <ul class=post-meta>
    
        
    
    <li class=publish-time><i class=icon-calendar></i>April 21, 2014</li>
    
        <li>&middot;</li>
        <li><a href="/tags/#outlier,-ref">#outlier,</a></li>
    
        <li>&middot;</li>
        <li><a href="/tags/#local-outlier,-ref">#local-outlier,</a></li>
    
        <li>&middot;</li>
        <li><a href="/tags/#kd-tree-ref">#kd-tree</a></li>
    
</ul>

    <h1 class=title-large>Improving performance of Local outlier factor with KD-Trees</h1>
    <div class=content>
        <br>
        <script src="//platform.linkedin.com/in.js" type="text/javascript">
        lang: en_US
        </script>
        <script type="IN/Share" data-counter="right"></script>
        <div class="g-plus" data-action="share" data-height="24"></div>
        <div class="fb-like" data-href="https://developers.facebook.com/docs/plugins/" data-layout="button_count" data-action="like" data-show-faces="true" data-share="true"></div>

        

        <p>Local outlier factor (LOF) is an outlier detection algorithm, that detects
outliers based on comparing local density of data instance with its neighbors.
It does so to decide if data instance belongs to region of similar density. It
can detect an outlier in a dataset, for which number of clusters is unknown, and
clusters are of different density and size. It's inspired from KNN (K-Nearest
Neighbors) algorithm, and is widely used. There is a <a href="http://www.rdatamining.com/examples/outlier-detection">R implemantation
available.</a></p>

<p>The naive approach to do this is to form all pair euclidan distance matrix, and
then run knn query to proceed further. But this approach just sucks, as it is
$\Theta(n^2)$ in terms of both space and time complexity. But, this can be
improvd with <a href="http://en.wikipedia.org/wiki/K-d_tree">KDTrees.</a>, and already its
<a href="http://scikit-learn.org/stable/modules/neighbors.html">implementation</a> exists
in python, thanks to scipy, so lets use this to find outliers.</p>

<h4 id="synthetic-dataset">Synthetic dataset</h4>

<div class="highlight"><pre><code class="python">%pylab inline
    import numpy as np
    np.random.seed(2) # to reproduce the result

    Populating the interactive namespace from numpy and matplotlib


    WARNING: pylab import has clobbered these variables: [&#39;dist&#39;]
    `%pylab --no-import-all` prevents importing * from pylab and numpy



    dim = 2 # number of dimensions of dataset = 2
    # cluster of normal random variable moderately dense
    data1 = np.random.np.random.multivariate_normal([0, 1500], [[100000, 0], [0, 100000]], 2000)
    
    # very dense
    data2 = np.random.np.random.multivariate_normal([2000, 0], [[10000, 0], [0, 10000]], 2500)
    
    # sparse
    data3 = np.random.np.random.multivariate_normal([2500, 2500], [[100000, 0], [0, 100000]], 500)
    
    # mix the three dataset and shuffle
    data = np.vstack((np.vstack((data1, data2)), data3))
    np.random.shuffle(data)
    
    # add some noise : zipf is skewed distribution and can have extreme values(outliers)
    zipf_alpha = 2.25
    noise = np.random.zipf(zipf_alpha, (5000,dim)) * np.sign((np.random.randint(2, size = (5000, dim)) - 0.5))
    data += noise</code></pre></div>

<h4 id="naive-approach-to-lof">Naive approach to LOF</h4>

<p>Pairwise Euclidean distance calculation with DistanceMetric implementation in
scikit-learn. In this, we just compute all-pair euclidean distance, i.e. $d(i,
j) = |x(i)-x(j)|_2$.</p>

<div class="highlight"><pre><code class="python">from sklearn.neighbors import DistanceMetric
     # distance between points
    import time
    tic = time.time()
    dist = DistanceMetric.get_metric(&#39;euclidean&#39;).pairwise(data)
    print &#39;++ took %g msecs for Distance computation&#39; %  ((time.time() - tic)* 1000)</code></pre></div>

<div class="highlight"><pre><code class="python">++ took 740 msecs for Distance computation</code></pre></div>

<p>Performing KNN query.In this step, the nearest k neighbors are identified
$N_k(i)$, and radius is the distance of k-th rearest neighbor of a datapoint.
<script type="math/tex"> r(i) = \max_{k \in N_k(i)} d(i, k) </script></p>

<div class="highlight"><pre><code class="python">tic = time.time()
    k = 17 # number of neighbors to consider
    # get the radius for each point in dataset (distance to kth nearest neighbor)
    # radius is the distance of kth nearest point for each point in dataset        
    idx_knn = np.argsort(dist, axis=1)[:,1 : k + 1] # by row&#39; get k nearest neighbour   
    radius = np.linalg.norm(data - data[idx_knn[:, -1]], axis = 1) # radius
    print &#39;+++ took %g msecs for KNN Querying&#39; %  ((time.time() - tic)* 1000)</code></pre></div>

<div class="highlight"><pre><code class="python">+++ took 4800 msecs for KNN Querying</code></pre></div>

<p>Then LRD(Local Reachability distance) is calculated. For this, first reach
distance $rd(i, j)$ is computed between point concern $x(i)$ and its neighbors $
j:j\in N_k(i)$, which is the maximum of euclidean distance or radius $r(i)$ of
point concerned. Then, LRD is the inverse of mean of reach distance of all
k-neighbors of each point.
<script type="math/tex"> rd(i, j) = \max{\{d(i, j), r(i) }\} for\ j\in N_k(i) </script>
<script type="math/tex">LRD(i) = \frac{|N_k(i)|}{ \sum_{j \in N_k(i) }{rd(i, j)}} </script></p>

<div class="highlight"><pre><code class="python"># calculate the local reachability density
    tic = time.time()
    LRD = []
    for i in range(idx_knn.shape[0]):
        LRD.append(np.mean(np.maximum(dist[i, idx_knn[i]], radius[idx_knn[i]])))
    print &#39;++++ took %g msecs for LRD computation&#39; %  ((time.time() - tic)* 1000)</code></pre></div>

<div class="highlight"><pre><code class="python">++++ took 429 msecs for LRD computation</code></pre></div>

<p>finally, the outlier score $LOF$ is calsulated.
<script type="math/tex"> LOF(i) = \frac { \sum_{j \in N_k(i)} {\frac{LRD(j)}{LRD(i)} }} { |N_k(i)|} </script></p>

<div class="highlight"><pre><code class="python"># calculating the outlier score
    tic = time.time()
    rho = 1. / np.array(LRD) # inverse of density
    outlier_score = np.sum(rho[idx_knn], axis = 1)/ np.array(rho, dtype = np.float16)
    outlier_score *= 1./k
    print &#39;+++++ took %g msecs for Outlier scoring&#39; %  ((time.time() - tic)* 1000)</code></pre></div>

<div class="highlight"><pre><code class="python">+++++ took 9.99999 msecs for Outlier scoring</code></pre></div>

<p>Now lets se the histogram of Outlier score, to choose the optimal threshold to
decid weather a data-point is outlier is not.</p>

<div class="highlight"><pre><code class="python">weights = np.ones_like(outlier_score)/outlier_score.shape[0] # to normalize the histogram to probability plot
    hist(outlier_score, bins = 50, weights = weights, histtype = &#39;stepfilled&#39;, color = &#39;cyan&#39;)
    title(&#39;Distribution of outlier score&#39;)</code></pre></div>

<div class="highlight"><pre><code class="python">&lt;matplotlib.text.Text at 0x36030588&gt;</code></pre></div>

<p><img src="/images/lof_files/lof_16_1.png" alt="png" /></p>

<p>It can be observd that, the optimal outlier score threshold to decide weather a
data-point is outlier is outlier or not is around 2 for most of the cases, so
lets use it to see our sesults.</p>

<div class="highlight"><pre><code class="python">threshold = 2.
    # plot non outliers as green
    scatter(data[:, 0], data[:, 1], c = &#39;green&#39;, s = 10, edgecolors=&#39;None&#39;, alpha=0.5)
    # find the outliers and plot te outliers
    idx = np.where(outlier_score &gt; threshold)
    scatter(data[idx, 0], data[idx, 1], c = &#39;red&#39;, s = 10, edgecolors=&#39;None&#39;, alpha=0.5)</code></pre></div>

<div class="highlight"><pre><code class="python">&lt;matplotlib.collections.PathCollection at 0x3640e6a0&gt;</code></pre></div>

<p><img src="/images/lof_files/lof_18_1.png" alt="png" /></p>

<p>We have seen the results of LOF with naive approachfor KNN queries. Now lets see
optimisations with KD-Trees.</p>

<h4 id="using-kd-trees">Using KD Trees</h4>

<p>KD-Trees insertion and KNN query.</p>

<div class="highlight"><pre><code class="python">from sklearn.neighbors import KDTree as Tree
    tic = time.time()
    BT = Tree(data, leaf_size=5, p=2)
    # Query for k nearest, k + 1 because one of the returnee is self
    dx, idx_knn = BT.query(data[:, :], k = k + 1)
    
    print &#39;++ took %g msecs for Tree KNN Querying&#39; %  ((time.time() - tic)* 1000)

    ++ took 122 msecs for Tree KNN Querying</code></pre></div>

<p>LRD computation.</p>

<div class="highlight"><pre><code class="python">tic = time.time()
    dx, idx_knn = dx[:, 1:], idx_knn[:, 1:]
    # get the radius for each point in dataset
    # radius is the distance of kth nearest point for each point in dataset        
    radius = dx[:, -1]
    # calculate the local reachability density
    LRD = np.mean(np.maximum(dx, radius[idx_knn]), axis = 1)
    
    print &#39;++ took %g msecs for LRD computation&#39; %  ((time.time() - tic)* 1000)

    ++ took 8.99982 msecs for LRD computation</code></pre></div>

<p>Now, rest is same, so, i'm just replicating the rsult for completion.</p>

<div class="highlight"><pre><code class="python"># calculating the outlier score
    tic = time.time()
    rho = 1. / np.array(LRD) # inverse of density
    outlier_score = np.sum(rho[idx_knn], axis = 1)/ np.array(rho, dtype = np.float16)
    outlier_score *= 1./k
    print &#39;+++++ took %g msecs for Outlier scoring&#39; %  ((time.time() - tic)* 1000)
    
    # plotiing the histogram of outlier score
    weights = np.ones_like(outlier_score)/outlier_score.shape[0] # to normalize the histogram to probability plot
    hist(outlier_score, bins = 50, weights = weights, histtype = &#39;stepfilled&#39;, color = &#39;cyan&#39;)
    title(&#39;Distribution of outlier score&#39;)
    
    #plotting the result
    threshold = 2.
    # plot non outliers as green
    figure()
    scatter(data[:, 0], data[:, 1], c = &#39;green&#39;, s = 10, edgecolors=&#39;None&#39;, alpha=0.5)
    # find the outliers and plot te outliers
    idx = np.where(outlier_score &gt; threshold)
    scatter(data[idx, 0], data[idx, 1], c = &#39;red&#39;, s = 10, edgecolors=&#39;None&#39;, alpha=0.5)

    +++++ took 4.00019 msecs for Outlier scoring

    &lt;matplotlib.collections.PathCollection at 0x36ad0b38&gt;</code></pre></div>

<p><img src="/images/lof_files/lof_26_2.png" alt="png" /></p>

<p><img src="/images/lof_files/lof_26_3.png" alt="png" /></p>

<p>The results are same, and should be.</p>

<h4 id="putting-everything-together">Putting everything together</h4>

<p>Lets create a class, to combine evrything together. It will be important in
evaluating performance. From above results, we note that the most time is spent
for KNN querying. Also, i will create a method to compare the performances and see the results.</p>

<script src="https://gist.github.com/bistaumanga/9207578.js" height="50"></script>

<p>Now, lets compare the performace of 2 methods- Naive and KDTree implementations.</p>

<div class="highlight"><pre><code class="python">perf_test(methods = [&#39;Tree&#39;, &#39;Naive&#39;], n_list = [2 ** i for i in range(4, 14)], plot = True)</code></pre></div>

<p><img src="/images/lof_files/lof_35_1.png" alt="png" /></p>

<p>We see that KDTree outperforms Naive method for narge $n$, but it may not do
well for small number of datasets. In my PC, i cannot run Naive method beyond
$2^{13}$ datapoints, or else i receie MemoryError. So, lets evauate te
performance of KDTrees upto 1Million datapoints.</p>

<div class="highlight"><pre><code class="python">perf_test(methods = [&#39;Tree&#39;], n_list = [2 ** i for i in range(4, 21)], plot = True)</code></pre></div>

<p><img src="/images/lof_files/lof_37_1.png" alt="png" /></p>

<p>We can see, algorithm is scaling well with data-set size $n$. If we analyse the
complexity of algorithm, its linearithmin , i.e. $\Theta (n\log{n})$.</p>

<h4 id="download-this-post-as">Download this post as:</h4>
<p><a href="/files/lof/lof.ipynb"><i class="fa fa-file-code-o fa-2x"></i> Ipython Notebook</a> | <a href="/files/lof/lof.pdf"><i class="fa fa-file-pdf-o fa-2x"></i> PDF</a></p>


    <p class=discuss>
        Discuss this post via <a href="mailto:bistaumanga@gmail.com">e-mail</a>
    </p>


</div>


<div id="disqus_thread"></div>
<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'bistaumanga'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        

    </div>

      </div>
    </div>
    <div class=pure-u-1-5>
      <div class=author-info>
    <img src="/images/author-image.jpg" class=author-image />
    <h1 class=author-name><a href=/>Umanga Bista</a></h1>
    <div class=nav>
        <a href="https://github.com/bistaumanga"><i class="fa fa-github-alt fa-2x"></i></a>
        <a href="mailto:bistaumanga@gmail.com"><i class="fa fa-envelope fa-2x"></i></a>
        <a href="https://np.linkedin.com/in/bistaumanga"><i class="fa fa-linkedin-square fa-2x"></a></i>
        <a href="https://www.researchgate.net/profile/Umanga_Bista"><i class="fa fa-flask fa-2x"></i></a>
    </div>
    
    <p class=author-bio>Research Assistant at <a href=http://www.logpoint.com/>LogPoint</a>.<br>
Formerly, Computer Engineering undergrad at <a href=http://www.ioe.edu.np/>Institute of Engineering, Central Campus</a>.
</p>
    
</div>
    </div>
  </div>
  <script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'bistaumanga'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
      var s = document.createElement('script'); s.async = true;
      s.type = 'text/javascript';
      s.src = '//' + disqus_shortname + '.disqus.com/count.js';
      (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
    </script>

    
    <div id="fb-root"></div>
    <script>(function(d, s, id) {
      var js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return;
      js = d.createElement(s); js.id = id;
      js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.0";
      fjs.parentNode.insertBefore(js, fjs);
    }(document, 'script', 'facebook-jssdk'));</script>

    <script type="text/javascript">
    (function() {
      var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
      po.src = 'https://apis.google.com/js/platform.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
    })();
    </script>
  </body>

  <div class="top-bar pure-g">
      <div class=pure-u-1-2>
        <h3> Copyright @ <a href = "/">Umanga Bista</a></h3>
      </div>
      <div class=pure-u-1-2>
        
      </div>

  </html>
