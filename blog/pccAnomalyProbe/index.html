<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name=description content="Personal website Umanga Bista">


  <link rel=stylesheet type=text/css href=/css/bootstrap.min.css>
  <!-- <link rel=stylesheet type=text/css href=/css/font-awesome.min.css> -->

  <!-- <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"> -->
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <title>
    
    Principal Component Analysis Based Unsupervised Anomaly Detection |
    
    Umanga Bista
  </title>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
  </script>

  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-50991800-1', 'bistaumanga.com.np');
  ga('send', 'pageview');

  </script>

  <link rel=stylesheet type=text/css href=/css/github.css>
  <link rel=stylesheet type=text/css href=/css/styles.css>
  <link rel=stylesheet type=text/css href=/css/highlight.css>

  

  </head>

  <body>

    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container-fluid">

    <div class="navbar-header">
      <button type="button" class="navbar-toggle hidden-xs" data-toggle="collapse" data-target=".navbar-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      <a class="navbar-brand visible-xs" href="/" style="padding: .2em .5em;"><img src="http://s.gravatar.com/avatar/05710f1aef9f9b4aed9b61381cd625db?s=40" class="img-circle" /></a>
      <a class="navbar-brand hidden-xs" href="/">UMB</a>

       <div class="navbar-header visible-xs" style="padding:0;">
        <a class="navbar-toggle nav-link" href="/tags" style="color:#8B7500; margin-right:0.1em;padding:0.2em;">
          <i class="fa fa-tags fa-fw"></i>Tags
        </a>

        <a class="navbar-toggle nav-link" href="https://www.twitter.com/bistaumanga" target="_blank" style="color:4099ff;margin-right:0.2em;padding:0.2em;">
        <i class="fa fa-twitter fa-lg"></i>
        </a>

        <a class="navbar-toggle nav-link" href="https://github.com/bistaumanga" target="_blank" style="color:#000;margin-right:0.2em;padding:0.2em;">
          <i class="fa fa-github-square fa-lg"></i>
        </a>

        <a class="navbar-toggle nav-link" href="https://np.linkedin.com/in/bistaumanga" target="_blank" style="color:#007bb6;margin-right:0.2em;padding:0.2em;">
          <i class="fa fa-linkedin-square fa-lg"></i>
        </a>

        <a class="navbar-toggle nav-link" href="mailto:mail@bistaumanga.com.np" target="_blank" style="color:#6E329D;margin-right:0.2em;padding:0.2em;">
          <i class="fa fa-envelope fa-lg"></i>
        </a>

        <a class="navbar-toggle nav-link" href="/blog" style="color:#8B7500; margin-right:0.1em;padding:0.2em;">
          <i class="fa fa-pencil-square fa-fw"></i>Blog
        </a>

<!--         <a class="navbar-toggle nav-link" href="/files/cv.pdf" style="color: #DC143C;;margin-right:0.25em;padding:0.2em;" target="_blank">
          <i class="fa fa-download fa-fw"></i>CV</a>
        </a> -->

        </a>
      </div>

    </div>

    <div class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="active"><a href="/"><i class="fa fa-home fa-lg"></i></a></li>
        <li><a href="/blog">Blog</a></li>
        <!-- <li><a href="/tags">Tags</a></li> -->
      </ul>
    </div>


  </div>

  </nav>


    <div class="col-sm-3 sidebar hidden-xs" style = "height: 100%;">
        <header class="sidebar-header">
    <img src="http://s.gravatar.com/avatar/05710f1aef9f9b4aed9b61381cd625db?s=160" class="img-circle" />
    <h3><a href=/>Umanga Bista<br>(उमङ्ग बिष्ट)</a></h3>
</header>
<div style = "color:#8B7500;" align = center>

 PhD Candidate in Machine Learning at the Australian National University.

</div>
<br>
<div align=center>
    <div class="btn-group">
        <a class="btn btn-default btn-sm" href="https://www.twitter.com/bistaumanga" style="color:#4099ff;" target="_blank">
            <i class="fa fa-twitter fa-2x"></i>
        </a>
        <a class="btn btn-default btn-sm" href="https://github.com/bistaumanga" style="color:#000;" target="_blank">
            <i class="fa fa-github-alt fa-2x"></i>
        </a>
        <a class="btn btn-default btn-sm" href="https://np.linkedin.com/in/bistaumanga" style="color:#007bb6;" target="_blank">
            <i class="fa fa-linkedin-square fa-2x"></i>
        </a>
        <a class="btn btn-default btn-sm" href="mailto:mail@bistaumanga.com.np" style="color:#6E329D;" target="_blank">
            <i class="fa fa-envelope fa-2x"></i>
        </a>
    </div>
    <br><br> 
    <a class="btn btn-default btn-sm" href="/files/cv.pdf" style="color: #DC143C;align: center;" target="_blank"><i class="fa fa-download fa-lg"></i>CV</a>
    <br><br>
    <a class="btn btn-link btn-sm" href="https://db.tt/rJjB3Llm" style="align: center;" target="_blank"> <i class="fa fa-dropbox fa-lg"></i> Free Online Storage </a>

    <br>
    <br>
    <div>
        <a href="https://twitter.com/bistaumanga" class="twitter-follow-button" data-show-count="false" data-size="large">
            Follow @bistaumanga
        </a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');
        </script>
    </div>

</div>

    </div>

    <div class="col-xs-18 col-sm-9 col-sm-offset-3" style="padding:1em;">
      <div class=post>
    
    <ul class=post-meta>
    
        
    
    <li><i class="fa fa-calendar fa-lg fa-fw" style="color:#ff2288;"></i>May 29, 2014</li>
    
        <li><i class="fa fa-tag fa-lg fa-fw" style="color:#28c;"></i><a href="/tags/#anomaly-ref">anomaly</a></li>
    
        <li><i class="fa fa-tag fa-lg fa-fw" style="color:#28c;"></i><a href="/tags/#principal-component-classifier-ref">principal-component-classifier</a></li>
    
        <li><i class="fa fa-tag fa-lg fa-fw" style="color:#28c;"></i><a href="/tags/#PCA-ref">PCA</a></li>
    
        <li><i class="fa fa-tag fa-lg fa-fw" style="color:#28c;"></i><a href="/tags/#unsupervised-ref">unsupervised</a></li>
    
</ul>

    <h1 class=title-large>Principal Component Analysis Based Unsupervised Anomaly Detection</h1>
    <div class=content>
        
        <br> 
        <script src="//platform.linkedin.com/in.js" type="text/javascript">
        lang: en_US
        </script>
        <script type="IN/Share" data-counter="right"></script>
        <!-- <div class="g-plus" data-action="share" data-height="24"></div> -->
        <!-- <div class="fb-like" rel="canonical" data-href= "" data-layout="standard" data-action="like" data-show-faces="true" data-share="true"></div> -->
        
        <a href="https://twitter.com/share" class="twitter-share-button" data-via="bistaumanga">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>

        <p>Unsupervised anomaly detection has its importance in the cases where we need to detect novilities from the unlabeled dataset of iids (independent and identically distributed). There has been different approaches to this problem such as Statistical Outlier Detection approaches e.g regression, gaussian density estimation, density based outlier detection e.g. <a href="/blog/lof/">Local Outlier Factor</a>, Kernel density estimation etc.. <a href="#charuoutlier">(Aggarwal, 2013)</a>.</p>

<h2 id="pca">PCA</h2>
<p><a href="http://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a> is a technique that transforms observations ($X \in \mathbb{R}^{n \ast d}$) of correlated variables in a higher dimensional space to the orthonormal space of uncorrelated variables. The new axis in the uncorrelated space of variables is given by <strong>eigenvactors</strong> and the amount of variance/energy retained along each eigenvector is given by <strong>eigenvalues</strong> in case of eigenvalue decomposition of <strong>covariance matrix</strong> of dataset. PCA can also be performed based on SVD and other decompositions.</p>

<script type="math/tex; mode=display">
\mu_i = \frac{1}{n}\sum_{j=1}^{n}X_{ji} \ ,\ for\ each\ i = 1\ to\ d \\
C_x = Cov(X) = (X-\mu)^T(X-mu) \\
C_x = V \Lambda V^{-1}
</script>

<p>where, $n$ is number of datapoints, and $d$ is dimensionality of observations.</p>

<p>$V \in \mathbb{R}^{d \ast d}$ is matrix of Eigen-Vectors of $C_x \in \mathbb{R}^{d \ast d}$, and $\Lambda \in \mathbb{R}^{d \ast d}$ is a diagonal matrix of Eigen-Values of $C_x$.</p>

<script type="math/tex; mode=display">% <![CDATA[
\Lambda = \begin{pmatrix}
\lambda_1 &0  &. &. &. &0\\
0 &\lambda_2  &. &. &. &0\\
 .&  .&  .& & &.\\
 .&  .&  & .& &.\\
 .&  .&  & & .&.\\
 .&  .&  .& .& .&\lambda_n\\
\end{pmatrix}  %]]></script>

<p>and eigenvalues are sorted and maximum information are retained along earlier eigen-vectors that the later ones, i.e.</p>

<script type="math/tex; mode=display">\lambda_1 > \lambda_2 > ... > \lambda_n </script>

<p>The percentage of energy retained in $r$ principal components after PCA transformation is given by:</p>

<script type="math/tex; mode=display"> \%\ energy\ retained = \frac{\sum_{i=1}^{r} \lambda_i^2}{\sum_{i=1}^{d} \lambda_i^2} </script>

<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>
<p>If we pick the $r$ principal components that maintain $p\%$ variance of data, the truncated matrix of eigen-vectors is $V_L \in \mathbb{R}^{d \ast r}$, matrix of eigenvalues is $\Lambda_L \in \mathbb{R}^{r \ast r}$.</p>

<p>To reduce the dimensionality means to transform the data from $\mathbb{R}^{d}$ space to $\mathbb{R}^{r}$ space or to project the data into the axes given by eigen-vectors, which can be accomplished by following equation:</p>

<script type="math/tex; mode=display">X_L = X.V_L </script>

<p>After this transform, the data will be transformed to a ortho-normal space where there is no correlation between variables.</p>

<p>Practically, this method often shows that dimensionality can often be reduced without much loss of information(typical choices are $1\%$, $5\%$).</p>

<p>I will use the concepts and try to demonstrate how they can be used to detect the <strong>portsweep</strong> attack in <a href="">KDD 1999</a> datasets.</p>

<h4 id="loading-the-datasets">Loading the datasets</h4>

<p>I have only selected few columns which were significant for the portsweep. These can be identified with Decision trees. ( rpart  function) or by manual inspection of ditribution of each variables for portsweep and normal data instances (boxlot(V13~V42, data = kdd)).</p>

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"portsweep."</span><span class="w">
</span><span class="n">cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">30</span><span class="p">,</span><span class="w"> </span><span class="m">27</span><span class="p">,</span><span class="w"> </span><span class="m">33</span><span class="p">,</span><span class="w"> </span><span class="m">34</span><span class="p">,</span><span class="w"> </span><span class="m">35</span><span class="p">,</span><span class="w"> </span><span class="m">36</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="m">12</span><span class="p">,</span><span class="w"> </span><span class="m">40</span><span class="p">,</span><span class="w"> </span><span class="m">23</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="c1"># type = "ipsweep."
# cols = c(37, 5, 6, 32, 8, 12, 24, 31, 34, 35)
</span><span class="n">kdd</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s1">'~/scripts/kddcup.data_10_percent'</span><span class="p">,</span><span class="w"> </span><span class="n">header</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span><span class="n">kdd.probe</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subset</span><span class="p">(</span><span class="n">kdd</span><span class="p">,</span><span class="n">V</span><span class="m">42</span><span class="w"> </span><span class="o">==</span><span class="w">  </span><span class="n">type</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">V</span><span class="m">42</span><span class="o">==</span><span class="s1">'normal.'</span><span class="p">,</span><span class="w">
                    </span><span class="n">select</span><span class="o">=</span><span class="n">cols</span><span class="p">)</span></code></pre></div>

<h4 id="pca-with-factominer">PCA with FactoMineR</h4>

<p>I will use PCA function of package <a href="">FactoMineR</a>.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">require</span><span class="p">(</span><span class="s1">'FactoMineR'</span><span class="p">)</span><span class="w">
</span><span class="n">nnn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ncol</span><span class="p">(</span><span class="n">kdd.probe</span><span class="p">)</span><span class="w"> </span><span class="c1">### number of variables
</span><span class="n">pca_result</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">PCA</span><span class="p">(</span><span class="n">kdd.probe</span><span class="p">,</span><span class="n">graph</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">F</span><span class="p">,</span><span class="w"> </span><span class="n">ncp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nnn</span><span class="p">,</span><span class="w"> </span><span class="n">scale.unit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">T</span><span class="p">)</span><span class="w">
</span><span class="n">pca.var</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pca_result</span><span class="o">$</span><span class="n">eig</span><span class="p">[[</span><span class="s1">'cumulative percentage of variance'</span><span class="p">]]</span><span class="o">/</span><span class="m">100</span><span class="w">
</span><span class="n">lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pca_result</span><span class="o">$</span><span class="n">eig</span><span class="p">[,</span><span class="w"> </span><span class="s1">'eigenvalue'</span><span class="p">]</span><span class="w">

</span><span class="n">require</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">qplot</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">pca.var</span><span class="p">),</span><span class="w"> </span><span class="n">pca.var</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Cumulative % variance retained"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"No. of Principal Components"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">theme_bw</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">geom_line</span><span class="p">(</span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ylim</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">)</span></code></pre></div>

<p><img src="/images/pcc_anomaly/pca.png" alt="plot of chunk pca" /></p>

<p>The plot of cumulative variance retained showed we can retain the must of the infomation among the first few axes.</p>

<h2 id="outlier-detection">Outlier Detection</h2>

<p>PCA can not only be used as dimensionality reduction technique, but also for outlier detection. The idea behind this is, that the anomalies will not conform with the reduced sub-space pattern of most of the normal-data, i.e. the loss for the outlier will be much more larger than the most of the normal instances in the dataset when first $r$ components are picked.</p>

<h3 id="chi2-test">1. $\chi^2$ Test</h3>

<p>Charu Aggarwal [Charu pp 59] has given a more accurate way of scoring the outlierness without picking the $r$ components, but rather using eigenvectors and eigenvalues to score the outlierness as normalized distance of datapoint to the centroid along the direction of each principal components.</p>

<table>
  <tbody>
    <tr>
      <td>$$Score(X_i) = \sum_{i=1}^{d}\frac{</td>
      <td>(X_i - \mu).V_i</td>
      <td>^2}{\lambda_i}$$</td>
    </tr>
  </tbody>
</table>

<p>In above relationship, the contribution of outlierness is backed by deviations from less significant principal components(with small $\lambda_i$).This value is $\chi^2$ distributed with $d$ degree of freedom.</p>

<p>So, an observation $X_i$ is an outlier if: <script type="math/tex"> Score(X_i) > \chi_d^2(\alpha)</script></p>

<p>for $d$ degree of freedom, and $1 - \alpha$ significance level.</p>

<p>For giving the outlier score,we can use above equation.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1">#anomaly_score &lt;- apply(pca_result$ind$coord, 1, function(x) sum(x ^ 2/lambda))
</span><span class="n">anomaly_score</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">as.matrix</span><span class="p">(</span><span class="n">pca_result</span><span class="o">$</span><span class="n">ind</span><span class="o">$</span><span class="n">coord</span><span class="p">)</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">lambda</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">))</span></code></pre></div>

<p>KDD dataset is labeled, so let me evaluate the performance of algorithm. For that we need threshold for decision of whether an observation is outlier. Since we know that this utlier score is $\chi^2$ distributed with $d$ degree of freedom, i will perform the $\chi^2$ test with various level of significance.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">significance</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="p">(</span><span class="m">0.9</span><span class="p">,</span><span class="w"> </span><span class="m">0.925</span><span class="p">,</span><span class="w"> </span><span class="m">0.95</span><span class="p">,</span><span class="w"> </span><span class="m">0.96</span><span class="p">,</span><span class="w"> </span><span class="m">0.97</span><span class="p">,</span><span class="w"> </span><span class="m">0.975</span><span class="p">,</span><span class="w"> </span><span class="m">0.98</span><span class="p">,</span><span class="w"> </span><span class="m">0.985</span><span class="p">,</span><span class="w"> </span><span class="m">0.9899</span><span class="p">,</span><span class="w"> </span><span class="m">0.99</span><span class="p">,</span><span class="w"> </span><span class="m">0.991</span><span class="p">,</span><span class="w"> </span><span class="m">0.992</span><span class="p">,</span><span class="w"> </span><span class="m">0.995</span><span class="p">,</span><span class="w"> </span><span class="m">0.999</span><span class="p">)</span><span class="w">
</span><span class="n">result</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">matrix</span><span class="p">(</span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">significance</span><span class="p">),</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">))</span><span class="w">
</span><span class="n">colnames</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'significance'</span><span class="p">,</span><span class="w"> </span><span class="s1">'precision'</span><span class="p">,</span><span class="w"> </span><span class="s1">'recall'</span><span class="p">,</span><span class="w"> </span><span class="s1">'F_measure'</span><span class="p">)</span><span class="w">

</span><span class="n">result_expected</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subset</span><span class="p">(</span><span class="n">subset</span><span class="p">(</span><span class="n">kdd</span><span class="p">,</span><span class="w"> </span><span class="n">V</span><span class="m">42</span><span class="w"> </span><span class="o">==</span><span class="w">  </span><span class="n">type</span><span class="o">|</span><span class="w"> </span><span class="n">V</span><span class="m">42</span><span class="o">==</span><span class="s1">'normal.'</span><span class="p">),</span><span class="w"> </span><span class="n">select</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="n">V</span><span class="m">42</span><span class="p">))</span><span class="w">
</span><span class="n">result_expected</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">result_expected</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="s2">"normal."</span><span class="w">

</span><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">significance</span><span class="p">)){</span><span class="w">
  </span><span class="n">sig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">significance</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w">
  </span><span class="n">thresh</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">qchisq</span><span class="p">(</span><span class="n">sig</span><span class="p">,</span><span class="w"> </span><span class="n">nnn</span><span class="p">)</span><span class="w">
  </span><span class="n">anomaly</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">anomaly_score</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">thresh</span><span class="w">
  </span><span class="n">result_obtained</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">as.factor</span><span class="p">(</span><span class="n">anomaly</span><span class="p">)</span><span class="w">
  </span><span class="n">crosstable</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">table</span><span class="p">(</span><span class="n">result_obtained</span><span class="p">,</span><span class="w"> </span><span class="n">result_expected</span><span class="p">)</span><span class="w">

  </span><span class="n">tp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">crosstable</span><span class="p">[</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="c1"># true_positive
</span><span class="w">  </span><span class="n">fp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">crosstable</span><span class="p">[</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="c1"># false positive
</span><span class="w">  </span><span class="n">fn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">crosstable</span><span class="p">[</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="c1"># false negative
</span><span class="w">
  </span><span class="n">precision</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tp</span><span class="o">/</span><span class="p">(</span><span class="n">tp</span><span class="o">+</span><span class="n">fp</span><span class="p">)</span><span class="w">
  </span><span class="n">recall</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tp</span><span class="o">/</span><span class="p">(</span><span class="n">tp</span><span class="o">+</span><span class="n">fn</span><span class="p">)</span><span class="w">
  </span><span class="n">f_measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">precision</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">recall</span><span class="w"> </span><span class="o">/</span><span class="p">(</span><span class="n">precision</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">recall</span><span class="p">)</span><span class="w">
  </span><span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">sig</span><span class="p">,</span><span class="w"> </span><span class="n">precision</span><span class="p">,</span><span class="w"> </span><span class="n">recall</span><span class="p">,</span><span class="w"> </span><span class="n">f_measure</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></code></pre></div>

<p>Now, i will plot the evaluation measures against the level of significance in $\chi^2$ test.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">significance</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">recall</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"recall"</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"recall"</span><span class="p">),</span><span class="w">
            </span><span class="n">lwd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'longdash'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">recall</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"recall"</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"recall"</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">F_measure</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"F-measure"</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"F-measure"</span><span class="p">),</span><span class="w">
            </span><span class="n">lwd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'dotdash'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">F_measure</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"F-measure"</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"F-measure"</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">precision</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Precision"</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Precision"</span><span class="p">),</span><span class="w">
            </span><span class="n">lwd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'solid'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">precision</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Precision"</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Precision"</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="o">+</span><span class="w">
  </span><span class="n">theme_bw</span><span class="p">()</span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s2">"Significance $1 - \alpha$"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ylab</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_colour_manual</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">))</span><span class="o">+</span><span class="w">
  </span><span class="n">scale_shape_manual</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="m">17</span><span class="p">,</span><span class="w"> </span><span class="m">18</span><span class="p">))</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span></code></pre></div>

<p><img src="/images/pcc_anomaly/results.png" alt="plot of chunk results" /></p>

<p>We can see the algorithm performs poorly, especially in terms of Precision, which means a lot of False alarm are generated. We would like to improve on this, for this we will explore the <strong>Principal component Classifier method</strong>.</p>

<h3 id="principal-component-classifier-pcc">2. Principal Component Classifier (PCC)</h3>

<p>PCC was first introduced by Shyu et. al.<a href="#pcc-anomaly">(Shyu, Chen, Sarinnapakorn, &amp; Chang, n.d.)</a>. They divided the principal components into two categories: <strong>Major Components</strong> which account for first $q\%$ of variance, and minor components, which accountfor the last $r\%$ of variance of the data.</p>

<p>To Be continued…</p>

<h2 id="references">References</h2>
<ol class="bibliography" style="font-size:12pt; text-align:left;"><li><span id="charuoutlier">Aggarwal, C. C. (2013). <i>Outlier Analysis</i> (3rd ed.). Springer. Retrieved from http://link.springer.com/book/10.1007/978-1-4614-6396-2</span></li>
<li><span id="pcc-anomaly">Shyu, M.-L., Chen, S.-C., Sarinnapakorn, K., &amp; Chang, L. W. A Novel Anomaly Detection Scheme based on Principal Component Classifier. Retrieved from http://users.cis.fiu.edu/~chens/PDF/ICDM03_WS.pdf</span></li></ol>


    <p class=discuss>
        Discuss this post via <a href="mailto:mail@bistaumanga.com.np">e-mail</a>
    </p>


</div>


<div id="disqus_thread"></div>
<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'bistaumanga'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        

    </div>

      <div align="center">
<hr/>
<i class="fa fa-copyright"></i>
<a href = "/">umb</a>, powered by <a href = "http://jekyllrb.com/" target="_blank">jekyll</a>, <a href = "http://getbootstrap.com/" target="_blank">bootstrap</a>, <a href = "https://pages.github.com/" target="_blank">gh-pages</a></div>
    </div>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

    <!-- Disqus -->
    <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'bistaumanga'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
    </script>
</body>

</html>
